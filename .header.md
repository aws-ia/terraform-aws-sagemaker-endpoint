# Terraform Amazon SageMaker Endpoint Module

<!-- markdownlint-disable MD012 -->
This module includes resources to deploy Amazon SageMaker endpoints. It takes care of creating a SageMaker model, SageMaker endpoint configuration, and the SageMaker endpoint.

With Amazon SageMaker, you can start getting predictions, or inferences, from your trained machine learning models. SageMaker provides a broad selection of ML infrastructure and model deployment options to help meet all your ML inference needs. With SageMaker Inference, you can scale your model deployment, manage models more effectively in production, and reduce operational burden.

This module supports the following ways to deploy a model, depending on your use case:

- For persistent, real-time endpoints that make one prediction at a time, use SageMaker real-time hosting services. Real-time inference is ideal for inference workloads where you have real-time, interactive, low latency requirements. You can deploy your model to SageMaker hosting services and get an endpoint that can be used for inference. These endpoints are fully managed and support autoscaling.
- For requests with large payload sizes up to 1GB, long processing times, and near real-time latency requirements, use Amazon SageMaker Asynchronous Inference. Amazon SageMaker Asynchronous Inference is a capability in SageMaker that queues incoming requests and processes them asynchronously. This option is ideal for requests with large payload sizes (up to 1GB), long processing times (up to one hour), and near real-time latency requirements. Asynchronous Inference enables you to save on costs by autoscaling the instance count to zero when there are no requests to process, so you only pay when your endpoint is processing requests.

## Model configuration

### Single container

In the event that a single container is sufficient for your inference use-case, you can define a single-container model.

### Inference pipeline

An inference pipeline is an Amazon SageMaker model that is composed of a linear sequence of multiple containers that process requests for inferences on data. See the [AWS documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/inference-pipelines.html) to learn more about SageMaker inference pipelines. To define an inference pipeline, you can provide additional containers for your model.

### Network isolation

If you enable network isolation, the containers can't make any outbound network calls, even to other AWS services such as Amazon Simple Storage Service (S3). Additionally, no AWS credentials are made available to the container runtime environment.

To enable network isolation, set the ***enable_network_isolation*** property to true.

### Container images

#### ECR Image

Reference an image available within ECR

### DLC Image

Reference an [AWS Deep Learning Container](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/what-is-dlc.html) image.

### Model artifacts

If you choose to decouple your model artifacts from your inference code (as is natural given different rates of change between inference code and model artifacts), the artifacts can be specified via the model_data_source property of var.containers. The default is to have no model artifacts associated with a model. For instance: model_data_source=s3://{bucket_name}/{key_name}/model.tar.gz

## Model hosting

Amazon SageMaker provides model hosting services for model deployment. Amazon SageMaker provides an HTTPS endpoint where your machine learning model is available to provide inferences.

### Endpoint configuration

### Endpoint

When this module creates an endpoint, Amazon SageMaker launches the ML compute instances and deploys the model as specified in the configuration. To get inferences from the model, client applications send requests to the Amazon SageMaker Runtime HTTPS endpoint.

#### Real-time inference endpoints

TODO

#### Asynchronous inference endpoints

Coming soon

### AutoScaling

To enable autoscaling on the production variant, use the autoscaling_config variable. For load testing guidance on determining the maximum requests per second per instance, please see this [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/endpoint-scaling-loadtest.html).
